{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pikepdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_pdf_password(input_pdf, output_pdf, password):\n",
    "    try:\n",
    "        # Open the encrypted PDF with the provided password\n",
    "        with pikepdf.open(input_pdf, password=password) as pdf:\n",
    "            # Save the PDF without the password\n",
    "            pdf.save(output_pdf)\n",
    "\n",
    "        print(f\"Password removed successfully. Saved to {output_pdf}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "input_pdf = \"pnb_statement.pdf\"  # Path to the input PDF\n",
    "output_pdf = f'1{input_pdf}'  # Path to the output PDF\n",
    "password = \"02172021000234\"  # Password for the encrypted PDF\n",
    "\n",
    "remove_pdf_password(input_pdf, output_pdf, password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=run_id):\n",
    "  # mlflow.log_param(\"row_count\",df_merged.count())\n",
    "  mlflow.log_dict(null_counts, \"null_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "# exp_id = client.create_experiment(\"/Users/i-sachin.bahuleyan@d2kindia.com/PD_modelsv6\")\n",
    "# experiment = client.get_experiment('2953816066572483')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.get_run('2a1f6d3066984315af874674efc29f37')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"artifacts/roc_curve/roc_curve\"+\".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_string = string.replace('\"', '').replace(\"'\", \"\").replace('`', '')\n",
    "\n",
    "feat = run.data.params['features'].replace('\"','').replace(\"'\",'').replace('[','').replace(']','').replace(\",\",'').split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"..\")\n",
    "\n",
    "from dashboard.components import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = helper.Helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obj.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "art = client.list_artifacts('5db55e7be5504734a98e6b2a45518ecc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "art[2].path.startswith('feature_importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if art[0].path in ['XGBClassifier', 'LogisticRegression', 'DtClassifier']:\n",
    "    # Do something\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = client.download_artifacts('d8efee6cdc2f40fa81508e5a7c43f367',art[2].path,dst_path='../dashboard/assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Test\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.s3a.access.key\", \"TYUFUFGRYT44XA5CJJQB\")\n",
    "spark.conf.set(\"fs.s3a.secret.key\", \"DB3AKW1PDSBGGXR7FNPMG7XMTT5E5DYZ6NI4H56Y\")\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"https://mum-objectstore.e2enetworks.net\")\n",
    "spark.conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark.conf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "# spark.conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.conf.set(\"spark.kryoserializer.buffer.max.mb\", \"2048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://d2k-datalake-raw-dev/on-prem-mssql-export/dena-bank/CustomerLoanDetailsForPD/*.parquet\").limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = '../dashboard/artifacts/LogisticRegression'\n",
    "\n",
    "loaded_model = mlflow.spark.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the pickle file\n",
    "file_path = \"../dashboard/assets/encoding_list.pkl\"\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['GL',\n",
    " 'Area',\n",
    " 'Caste',\n",
    " 'Scheme',\n",
    " 'Activity',\n",
    " 'District',\n",
    " 'Religion',\n",
    " 'SubSector',\n",
    " 'Occupation',\n",
    " 'FORMATGROUP',\n",
    " 'Constitution',\n",
    " 'FACILITYTYPE',\n",
    " 'CUSTOMERSTATUS',\n",
    " 'FACILITIESSTATUS',\n",
    " ]\n",
    "\n",
    "\n",
    "numerical_cols = ['Age',\n",
    " 'CAD',\n",
    " 'CADU',\n",
    " 'UsedRv',\n",
    " 'Balance',\n",
    " 'Product',\n",
    " 'TotOsNF',\n",
    " 'AdhocAmt',\n",
    " 'InttRate',\n",
    " 'InttType',\n",
    " 'TotalAdv',\n",
    " 'AppGovGur',\n",
    " 'CURQTRINT',\n",
    " 'GLProduct',\n",
    " 'PRVQTRINT',\n",
    " 'RvPrimary',\n",
    " 'Unsecured',\n",
    " 'TotLimitNF',\n",
    " 'CoverGovGur',\n",
    " 'DepValPlant',\n",
    " 'ACCTotalProv',\n",
    " 'BaselCatMark',\n",
    " 'CURRENTLIMIT',\n",
    " 'CurQtrCredit',\n",
    " 'DRAWINGPOWER',\n",
    " 'PREQTRCREDIT',\n",
    " 'UnAdjSubSidy',\n",
    " 'UnappliedInt',\n",
    " 'ProvUnsecured',\n",
    " 'TotalWriteOff',\n",
    " 'AppropriatedRv',\n",
    " 'OrgCostOfEquip',\n",
    " 'TotLimitFunded',\n",
    " 'Cust_AssetClass',\n",
    " 'LimitSanctioned',\n",
    " 'PriCashSecurity',\n",
    " 'CollCashSecurity',\n",
    " 'CollNonCRMSecurity',\n",
    " 'OrgCostOfPlantMech']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "columns = OrderedDict()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    for data_item in data:\n",
    "        if col in data_item:\n",
    "            # Extract the values from the dictionary\n",
    "            values = list(data_item[col].values())\n",
    "            # Ensure all lists are of the same length by padding with None (or NaN)\n",
    "            max_length = max(len(columns.get(k, [])) for k in columns) if columns else len(values)\n",
    "            for k in columns:\n",
    "                # Pad existing columns to match the new max length\n",
    "                columns[k] += [None] * (max_length - len(columns[k]))\n",
    "            # Add the new column\n",
    "            columns[col] = values + [None] * (max_length - len(values))\n",
    "\n",
    "# Handle any remaining columns that need padding\n",
    "max_length = max(len(v) for v in columns.values())\n",
    "for k in columns:\n",
    "    columns[k] += [None] * (max_length - len(columns[k]))\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_options = [\n",
    "    {col: list(data_item[col].values())}\n",
    "    for col in categorical_cols\n",
    "    for data_item in data\n",
    "    if col in data_item\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropdown_options[1]['Area'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = {'GL': 'TL - - RETAIL SCHEMES ', 'Area': 'URBAN', 'Caste': 'OBC', 'Scheme': 'DENA GRADING & SORTING', 'Activity': 'PROCESSING OF PULSES E.G.  DAL MILLS', 'District': 'INDORE', 'Religion': 'HINDU', 'SubSector': 'SERVICES - MICRO ENTERPRISES-NPS', 'Occupation': 'SURVEYORS / ACTUARIES', 'FORMATGROUP': 'C', 'Constitution': 'CENTRAL GOVERNMENT', 'FACILITYTYPE': 'DL', 'CUSTOMERSTATUS': 'STD', 'FACILITIESSTATUS': 'STD'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_encodings(categorical_columns, data):\n",
    "\n",
    "    encoding = {}\n",
    "\n",
    "    for col, value in categorical_columns.items():\n",
    "        for index, encoding_dict in enumerate(data):\n",
    "            if col in encoding_dict:\n",
    "                for key, val in encoding_dict[col].items():\n",
    "                    if val == value:\n",
    "                        mean_encoding_key = f\"{col}_mean_encoding\"\n",
    "                        encoding[col] = data[index][mean_encoding_key][key]\n",
    "                                \n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = find_encodings(categorical_columns, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_numerical_columns(numerical_columns,df):\n",
    "\n",
    "    # Assemble numerical columns into a single vector\n",
    "    assembler = VectorAssembler(inputCols=list(numerical_columns.keys()), outputCol=\"features\")\n",
    "    assembled_df = assembler.transform(df)\n",
    "\n",
    "    # Apply StandardScaler\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scaler_model = scaler.fit(assembled_df)\n",
    "    scaled_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "    # Extract scaled values into a dictionary\n",
    "    transformed_values = {}\n",
    "    scaled_data = scaled_df.select(\"scaledFeatures\").collect()\n",
    "    for i, col in enumerate(numerical_columns.keys()):\n",
    "        transformed_values[col] = scaled_data[0][\"scaledFeatures\"][i]\n",
    "\n",
    "    return transformed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numerical_columns = {'Age': 30, 'CAD': 0, 'CADU': 0, 'UsedRv': 0, 'Balance': 0, 'Product': 0, 'TotOsNF': 0, 'AdhocAmt': 0, 'InttRate': 0, 'InttType': 0, 'TotalAdv': 0, 'AppGovGur': 0, 'CURQTRINT': 0, 'GLProduct': 0, 'PRVQTRINT': 0, 'RvPrimary': 0, 'Unsecured': 0, 'TotLimitNF': 0, 'CoverGovGur': 0, 'DepValPlant': 0, 'ACCTotalProv': 0, 'BaselCatMark': 0, 'CURRENTLIMIT': 0, 'CurQtrCredit': 0, 'DRAWINGPOWER': 0, 'PREQTRCREDIT': 0, 'UnAdjSubSidy': 0, 'UnappliedInt': 0, 'ProvUnsecured': 0, 'TotalWriteOff': 0, 'AppropriatedRv': 0, 'OrgCostOfEquip': 0, 'TotLimitFunded': 0, 'Cust_AssetClass': 0, 'LimitSanctioned': 0, 'PriCashSecurity': 0, 'CollCashSecurity': 0, 'CollNonCRMSecurity': 0, 'OrgCostOfPlantMech': 0}\n",
    "\n",
    "df = spark.createDataFrame([numerical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_values = transform_numerical_columns(numerical_columns, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = {**encodings,**transformed_values}\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_typed = {key: float(value) if isinstance(value, (int, float)) else value for key, value in final_data.items()}\n",
    "final_data_typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a PySpark DataFrame\n",
    "final_df = spark.createDataFrame([final_data_typed])\n",
    "# final_df = spark.createDataFrame([final_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['GL',\n",
    " 'Area',\n",
    " 'Caste',\n",
    " 'Scheme',\n",
    " 'Activity',\n",
    " 'District',\n",
    " 'Religion',\n",
    " 'SubSector',\n",
    " 'Occupation',\n",
    " 'FORMATGROUP',\n",
    " 'Constitution',\n",
    " 'FACILITYTYPE',\n",
    " 'CUSTOMERSTATUS',\n",
    " 'FACILITIESSTATUS',\n",
    " ]\n",
    "\n",
    "\n",
    "numerical_cols = ['Age',\n",
    " 'CAD',\n",
    " 'CADU',\n",
    " 'UsedRv',\n",
    " 'Balance',\n",
    " 'Product',\n",
    " 'TotOsNF',\n",
    " 'AdhocAmt',\n",
    " 'InttRate',\n",
    " 'InttType',\n",
    " 'TotalAdv',\n",
    " 'AppGovGur',\n",
    " 'CURQTRINT',\n",
    " 'GLProduct',\n",
    " 'PRVQTRINT',\n",
    " 'RvPrimary',\n",
    " 'Unsecured',\n",
    " 'TotLimitNF',\n",
    " 'CoverGovGur',\n",
    " 'DepValPlant',\n",
    " 'ACCTotalProv',\n",
    " 'BaselCatMark',\n",
    " 'CURRENTLIMIT',\n",
    " 'CurQtrCredit',\n",
    " 'DRAWINGPOWER',\n",
    " 'PREQTRCREDIT',\n",
    " 'UnAdjSubSidy',\n",
    " 'UnappliedInt',\n",
    " 'ProvUnsecured',\n",
    " 'TotalWriteOff',\n",
    " 'AppropriatedRv',\n",
    " 'OrgCostOfEquip',\n",
    " 'TotLimitFunded',\n",
    " 'Cust_AssetClass',\n",
    " 'LimitSanctioned',\n",
    " 'PriCashSecurity',\n",
    " 'CollCashSecurity',\n",
    " 'CollNonCRMSecurity',\n",
    " 'OrgCostOfPlantMech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_order = numerical_cols + categorical_cols\n",
    "desired_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.select([col for col in desired_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_assembler = VectorAssembler(inputCols=final_features, outputCol=\"features\")\n",
    "what_if_data = final_assembler.transform(final_df)\n",
    "pred = loaded_model.transform(what_if_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_list = pred.select(\"probability\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(probability_list[0][1],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count = client.download_artifacts('2a1f6d3066984315af874674efc29f37',art[2].path,dst_path='../dashboard/artifacts',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('../dashboard/artifacts/first_five_rows.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[\"data\"], columns=data[\"columns\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(data.items()), columns=[\"column\", \"null_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, count in data.items():\n",
    "    print(column,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "    \n",
    "# Print the values as a dictionary\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['run_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Test\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "logged_model = '../dashboard/artifacts/LogisticRegression'\n",
    "\n",
    "loaded_model = mlflow.spark.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Path to the pickle file\n",
    "file_path = \"../dashboard/assets/encoding_list.pkl\"\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['GL',\n",
    " 'Area',\n",
    " 'Caste',\n",
    " 'Scheme',\n",
    " 'Activity',\n",
    " 'District',\n",
    " 'Religion',\n",
    " 'SubSector',\n",
    " 'Occupation',\n",
    " 'FORMATGROUP',\n",
    " 'Constitution',\n",
    " 'FACILITYTYPE',\n",
    " 'CUSTOMERSTATUS',\n",
    " 'FACILITIESSTATUS',\n",
    " ]\n",
    "\n",
    "\n",
    "numerical_cols = ['Age',\n",
    " 'CAD',\n",
    " 'CADU',\n",
    " 'UsedRv',\n",
    " 'Balance',\n",
    " 'Product',\n",
    " 'TotOsNF',\n",
    " 'AdhocAmt',\n",
    " 'InttRate',\n",
    " 'InttType',\n",
    " 'TotalAdv',\n",
    " 'AppGovGur',\n",
    " 'CURQTRINT',\n",
    " 'GLProduct',\n",
    " 'PRVQTRINT',\n",
    " 'RvPrimary',\n",
    " 'Unsecured',\n",
    " 'TotLimitNF',\n",
    " 'CoverGovGur',\n",
    " 'DepValPlant',\n",
    " 'ACCTotalProv',\n",
    " 'BaselCatMark',\n",
    " 'CURRENTLIMIT',\n",
    " 'CurQtrCredit',\n",
    " 'DRAWINGPOWER',\n",
    " 'PREQTRCREDIT',\n",
    " 'UnAdjSubSidy',\n",
    " 'UnappliedInt',\n",
    " 'ProvUnsecured',\n",
    " 'TotalWriteOff',\n",
    " 'AppropriatedRv',\n",
    " 'OrgCostOfEquip',\n",
    " 'TotLimitFunded',\n",
    " 'Cust_AssetClass',\n",
    " 'LimitSanctioned',\n",
    " 'PriCashSecurity',\n",
    " 'CollCashSecurity',\n",
    " 'CollNonCRMSecurity',\n",
    " 'OrgCostOfPlantMech']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dashboard/assets/200_sample.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_encodings(categorical_columns, data):\n",
    "\n",
    "    encoding = {}\n",
    "\n",
    "    for col, value in categorical_columns.items():\n",
    "        for index, encoding_dict in enumerate(data):\n",
    "            if col in encoding_dict:\n",
    "                for key, val in encoding_dict[col].items():\n",
    "                    if val == value:\n",
    "                        mean_encoding_key = f\"{col}_mean_encoding\"\n",
    "                        encoding[col] = data[index][mean_encoding_key][key]\n",
    "                                \n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_list = []\n",
    "for i in range(len(df)):\n",
    "    encoding = find_encodings(df.iloc[i], data)\n",
    "    encoding_list.append(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = pd.DataFrame(encoding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_numerical_columns(numerical_columns,df):\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=list(numerical_columns.keys()), outputCol=\"features\")\n",
    "    assembled_df = assembler.transform(df)\n",
    "\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "    scaler_model = scaler.fit(assembled_df)\n",
    "    scaled_df = scaler_model.transform(assembled_df)\n",
    "\n",
    "    transformed_values = {}\n",
    "    scaled_data = scaled_df.select(\"scaledFeatures\").collect()\n",
    "    for i, col in enumerate(numerical_columns.keys()):\n",
    "        transformed_values[col] = scaled_data[0][\"scaledFeatures\"][i]\n",
    "\n",
    "    return transformed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df[numerical_cols]\n",
    "numerical_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df[numerical_cols]\n",
    "df = spark.createDataFrame(numerical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_values = transform_numerical_columns(numerical_columns, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def apply_standard_scaler(df: DataFrame, input_cols: list, output_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Applies StandardScaler to the specified columns in a PySpark DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input PySpark DataFrame.\n",
    "    - input_cols (list): List of column names to scale.\n",
    "    - output_col (str): The name of the output column containing the scaled values.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A new DataFrame with the scaled values in the specified output column.\n",
    "    \"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "    \n",
    "    # Combine input columns into a single vector column\n",
    "    assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features_vector\")\n",
    "    df = assembler.transform(df)\n",
    "    \n",
    "    # Initialize the StandardScaler\n",
    "    scaler = StandardScaler(inputCol=\"features_vector\", outputCol=output_col, withMean=True, withStd=True)\n",
    "    \n",
    "    # Fit and transform the DataFrame\n",
    "    scaler_model = scaler.fit(df)\n",
    "    scaled_df = scaler_model.transform(df)\n",
    "    \n",
    "    # Return the resulting DataFrame\n",
    "    return scaled_df.select(*df.columns, output_col)\n",
    "\n",
    "scaled_df = apply_standard_scaler(df, input_cols=numerical_cols, output_col=\"scaled_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.select('scaled_features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_df = spark.createDataFrame(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define the model's accuracy\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define the metrics and their values\n",
    "metrics = ['F1 Score', 'FPR', 'TPR', 'Precision', 'ROC AUC']\n",
    "values = [0.85, 0.12, 0.90, 0.88, 0.92]  # Replace with your values\n",
    "\n",
    "# Ensure the radar plot is closed by repeating the first value\n",
    "values += [values[0]]  # Close the loop for the radar chart\n",
    "metrics += [metrics[0]]  # Close the loop for the radar chart\n",
    "\n",
    "# Create the radar plot\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=values,\n",
    "    theta=metrics,\n",
    "    fill='toself',\n",
    "    name='Model Metrics'\n",
    "))\n",
    "\n",
    "# Update the layout for better visualization\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 1]  # Assuming metric values range from 0 to 1\n",
    "        )\n",
    "    ),\n",
    "    title=\"Radar Plot of Model Metrics\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = '../dashboard/assets/roc_curve_XGBClassifier.json'\n",
    "with open(path, 'r') as file:\n",
    "        roc_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 300\n",
    "fpr_reduced = roc_data[\"fpr\"][::step]\n",
    "tpr_reduced = roc_data[\"tpr\"][::step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fpr_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "path = '../dashboard/assets/roc_curve_LogisticRegression.json'\n",
    "with open(path, 'r') as file:\n",
    "    roc_data = json.load(file)\n",
    "step = 300\n",
    "fpr_reduced = roc_data[\"fpr\"][::step]\n",
    "tpr_reduced = roc_data[\"tpr\"][::step]\n",
    "# Plot the ROC curve\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the line for ROC\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=fpr_reduced,\n",
    "        y=tpr_reduced,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"ROC Curve\",\n",
    "        line=dict(color=\"blue\", width=2),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add layout details\n",
    "fig.update_layout(\n",
    "    title=\"ROC Curve\",\n",
    "    xaxis_title=\"False Positive Rate (FPR)\",\n",
    "    yaxis_title=\"True Positive Rate (TPR)\",\n",
    "    xaxis=dict(range=[0, 1], showgrid=True),\n",
    "    yaxis=dict(range=[0, 1], showgrid=True),\n",
    "    template=\"plotly_white\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dashboard/assets/feature_importances_LogisticRegression.json'\n",
    "with open(path, 'r') as file:\n",
    "    feature_importance_dict = json.load(file)\n",
    "\n",
    "feature_importance_dict['coefficients']\n",
    "# abs(feature_importance_dict['coefficients']['UsedRv'])\n",
    "# sorted_features = sorted(feature_importance_dict['coefficients'].items(), key=lambda x: x[1], reverse=True)\n",
    "# sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../dashboard/assets/feature_importances_LogisticRegression.json'\n",
    "with open(path, 'r') as file:\n",
    "    feature_importance_dict = json.load(file)\n",
    "# Sort the dictionary by importance values in descending order\n",
    "sorted_features = sorted(feature_importance_dict['coefficients'].items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Extract the top 10 features and their corresponding importance values\n",
    "top_10_features = sorted_features[:10]\n",
    "top_10_feature_names = [item[0] for item in top_10_features]\n",
    "top_10_importance_values = [item[1] for item in top_10_features]\n",
    "top_10_feature_names = top_10_feature_names[::-1]\n",
    "top_10_importance_values = top_10_importance_values[::-1]\n",
    "\n",
    "# Create the horizontal bar chart using Plotly\n",
    "fig = go.Figure(go.Bar(\n",
    "    x=top_10_importance_values,\n",
    "    y=top_10_feature_names,\n",
    "    orientation='h',  # Horizontal bars\n",
    "    marker=dict(color='skyblue')\n",
    "))\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    title=\"Top 10 Feature Importances\",\n",
    "    xaxis_title=\"Importance\",\n",
    "    yaxis_title=\"Features\",\n",
    "    template=\"plotly_dark\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Test\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model_path = '../dashboard/artifacts/XGBClassifier'\n",
    "loaded_model = mlflow.spark.load_model(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
